import threading
from typing import Dict, Any, List
import abc
import concurrent.futures
import logging


class BaseScraperHandler(metaclass=abc.ABCMeta):
    """
    –ë–∞–∑–æ–≤—ã–π –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–∫—Ä–∞–ø–ø–µ—Ä–∞.
    –°–æ–¥–µ—Ä–∂–∏—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
      - scrape_category_previews
      - scrape_all_categories
    """

    def __init__(self, source_info: Dict[str, Any]):
        """
        :param source_info: –î–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–∏–∑ content_sources).
        """
        self.source_info = source_info
        self.source_id = source_info['id']
        self.driver_pool = None

    """ –ë–∞–∑–æ–≤—ã–π —Å–∫—Ä–∞–ø–ø–µ—Ä, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Selenium-–¥—Ä–∞–π–≤–µ—Ä–æ–≤. """

    def configure(self, driver_pool):
        """ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–∫—Ä–∞–ø–ø–µ—Ä: –ø–µ—Ä–µ–¥–∞—ë—Ç –ø—É–ª –¥—Ä–∞–π–≤–µ—Ä–æ–≤ –∏ id –∏—Å—Ç–æ—á–Ω–∏–∫–∞. """
        self.driver_pool = driver_pool

    @abc.abstractmethod
    def scrape_category_previews(self, category_slug: str) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–≤—å—é:
        [
          {
            "link": str,
            "image_link": str,
            "categories": str,
            "title": str
          },
          ...
        ]
        """
        pass

    def scrape_all_categories(self, categories: List[str]) -> List[Dict[str, Any]]:
        """
        –î–µ—Ñ–æ–ª—Ç–Ω–∞—è (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è) —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –æ—á–µ—Ä–µ–¥–∏.
        –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –Ω–∞—Å–ª–µ–¥–Ω–∏–∫ –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —ç—Ç–æ—Ç –º–µ—Ç–æ–¥.
        """
        all_data = []
        for cat in categories:
            cat_data = self.scrape_category_previews(cat)
            all_data.extend(cat_data)
        return all_data

    def scrape_articles(self, links: list[str]) -> list[dict]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
        """
        if not links:
            logging.info("üì≠ –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏ –ø—É—Å—Ç ‚Äî –Ω–µ—á–µ–≥–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å.")
            return []

        logging.info(f"üìÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(links)} —Å—Ç–∞—Ç–µ–π.")

        results = []
        lock = threading.Lock()

        def task(link: str):
            article = self.scrape_article(link)
            if article:
                with lock:
                    results.append(article)

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.driver_pool.max_drivers) as executor:
            executor.map(task, links)

        logging.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(results)} —Å—Ç–∞—Ç–µ–π.")
        return results

    @abc.abstractmethod
    def scrape_article(self, link: str) -> dict | None:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
        {link, title, text, sources}
        """
        pass
